{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:47:52.498895Z",
     "start_time": "2017-09-14T07:47:52.483390Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import dok_matrix\n",
    "from stop_words import get_stop_words\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from nltk import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# 시간을 30분 단위로 반올림하는 함수\n",
    "def ceil_dt(dt, minutes=30):\n",
    "    delta = timedelta(minutes=minutes)\n",
    "    return dt + (datetime.min - dt) % delta\n",
    "\n",
    "# wordnet lemmatizer - 복수형을 단수형으로 바꿔줌\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:47:58.940790Z",
     "start_time": "2017-09-14T07:47:53.288068Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"twit_new.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 각 트윗이 발생한 시간들 수집\n",
    "times = set()\n",
    "for twit in data:\n",
    "    time = ceil_dt(parse(twit['postedTime']).replace(tzinfo=None), minutes=30)\n",
    "    times.add(time)\n",
    "\n",
    "times = sorted(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:48:14.609469Z",
     "start_time": "2017-09-14T07:48:04.322558Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_twits = defaultdict(list)\n",
    "tweets_original = []\n",
    "voca = set()\n",
    "\n",
    "for twit in data:\n",
    "    # 시간을 30분 단위로 반올림\n",
    "    time = ceil_dt(parse(twit['postedTime']).replace(tzinfo=None), minutes=30)\n",
    "\n",
    "    # 단어가 3개 이상인 트윗만 취급\n",
    "    body = twit['body']\n",
    "    body = body.split()\n",
    "    if len(body) > 3:\n",
    "        # lemmatize\n",
    "        body = [wnl.lemmatize(t) for t in body if t.isalpha()]\n",
    "        voca.update(body)\n",
    "        body = \" \".join(body)\n",
    "\n",
    "        # 각 트윗을 시간에 따라 묶음\n",
    "        times_twits[time].append(body)\n",
    "        # 문서 검색용으로 별도로 저장\n",
    "        tweets_original.append(twit['body'])\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:48:27.513985Z",
     "start_time": "2017-09-14T07:48:27.498146Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stopwords 제거\n",
    "stopwords = set(get_stop_words('en'))\n",
    "stopwords.update(['via', 'will', 'just', 'one', 'don', 'lol', 'yes', 'doe'])\n",
    "voca = {v for v in voca if len(v) > 2}\n",
    "voca = list(voca - stopwords)\n",
    "voca_id = {w: i for i, w in enumerate(voca)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:48:40.307367Z",
     "start_time": "2017-09-14T07:48:29.835700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 각 시간별 term document matrix 생성\n",
    "tdm = dok_matrix((len(times), len(voca)), dtype=np.float32)\n",
    "for i, time in enumerate(times):\n",
    "    for twit in times_twits[time]:\n",
    "        for word in twit.split():\n",
    "            try:\n",
    "                tdm[i, voca_id[word]] += 1\n",
    "            except:\n",
    "                # stopwords\n",
    "                continue\n",
    "\n",
    "tdm_ = normalize(tdm)\n",
    "print(tdm_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:48:49.395517Z",
     "start_time": "2017-09-14T07:48:46.164553Z"
    }
   },
   "outputs": [],
   "source": [
    "# NMF\n",
    "K = 10\n",
    "n_words = 30\n",
    "nmf = NMF(n_components=K, init='nndsvd', max_iter=500, alpha=0.1)\n",
    "W = nmf.fit_transform(tdm_)\n",
    "H = nmf.components_\n",
    "\n",
    "for k in range(K):\n",
    "    print(f\"{k}th topic\")\n",
    "    for index in H[k].argsort()[::-1][:n_words]:\n",
    "        print(voca[index], end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:49:44.256504Z",
     "start_time": "2017-09-14T07:49:42.853923Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.models import HoverTool\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "output_notebook()\n",
    "\n",
    "W_ = W.T  # 시간별 토픽 가중치\n",
    "times_str = [str(t) for t in times]  # x축에 표시할 시간 문자열\n",
    "\n",
    "# 사용할 툴들\n",
    "tools_to_show = 'hover,box_zoom,pan,save,resize,reset,wheel_zoom'\n",
    "p = figure(\n",
    "    plot_width=800,\n",
    "    plot_height=500,\n",
    "    x_axis_type=\"datetime\",\n",
    "    tools=tools_to_show)\n",
    "\n",
    "for k in range(K):\n",
    "    # 각 토픽별 그래프에 추가하도록 source data 생성\n",
    "    source = ColumnDataSource(data={\n",
    "        'x': times,  # x축\n",
    "        'v': W_[k],  # 해당 토픽 가중치\n",
    "        'x_str': times_str,  # x축 문자열\n",
    "        'name': [str(k) for _ in range(W_.shape[1])]  # 토픽 번호를 각 시간마다 알려줌\n",
    "    })\n",
    "    p.line('x', 'v', source=source, legend=str(k), color=Category20[K][k])\n",
    "\n",
    "# 몇가지 interaction\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy = \"hide\"\n",
    "hover = p.select(dict(type=HoverTool))\n",
    "hover.tooltips = [(\"value\", \"@v\"), (\"topic\", \"@name\"), (\"date\", \"@x_str\")]\n",
    "hover.mode = 'mouse'\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:49:55.917189Z",
     "start_time": "2017-09-14T07:49:54.942496Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 각 단어 가중치\n",
    "word_freq = {}\n",
    "for i, word in enumerate(voca):\n",
    "    word_freq[word] = H[9, i]\n",
    "\n",
    "# wordcloud 생성\n",
    "wordcloud = WordCloud().generate_from_frequencies(word_freq)\n",
    "\n",
    "# wordcloud를 matplotlib을 이용해서 출력\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:51:56.710060Z",
     "start_time": "2017-09-14T07:51:42.769704Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 검색을 할 때 모든 문장에 대해 단어를 검색하는 것은 매우 느림!\n",
    "# term document matrix를 만들어서 해당 단어를 포함하는 문장을 바로 찾자\n",
    "tdm2 = dok_matrix((len(tweets_original), len(voca)), dtype=np.uint16)\n",
    "for i, tweet in enumerate(tweets_original):\n",
    "    tweet = [wnl.lemmatize(t) for t in tweet.split() if t.isalpha()]\n",
    "    for word in tweet:\n",
    "        try:\n",
    "            tdm2[i, voca_id[word]] += 1\n",
    "        except:\n",
    "            # stopwords\n",
    "            continue\n",
    "\n",
    "tdm2 = tdm2.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-14T07:52:01.315419Z",
     "start_time": "2017-09-14T07:51:58.586089Z"
    }
   },
   "outputs": [],
   "source": [
    "query = input(\"input keyword:\")  # 검색할 단어 인풋을 받음\n",
    "# 해당 단어(query)의 빈도가 0이 아닌 모든 문장을 찾아(nonzero()) 그 중 20개 트윗을 반환\n",
    "for i in tdm2[:, voca_id[query]].nonzero()[0][:20]:\n",
    "    print(tweets_original[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
